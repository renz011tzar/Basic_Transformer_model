{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V\n",
      " [[-0.54636056 -0.15054645  0.25871746  0.59966612 -1.54475043 -0.67852663\n",
      "  -1.53219383  1.15456754]\n",
      " [-1.79208351 -1.21408589 -1.02846623 -1.34557521  0.27998331  1.27477948\n",
      "  -0.11694454 -0.14395123]\n",
      " [-0.36151134  0.83264214  1.40804037 -1.25752191 -1.81624049  0.58136874\n",
      "  -1.88049262  0.92802504]\n",
      " [ 0.93952905 -1.95460459 -0.93223817 -0.62001563 -0.53428377 -0.3655168\n",
      "  -1.52143447 -0.15949322]]\n"
     ]
    }
   ],
   "source": [
    "L, d_k, d_v = 4,8,8\n",
    "\n",
    "q=np.random.randn(L,d_k)\n",
    "k=np.random.randn(L,d_k)\n",
    "v=np.random.randn(L,d_v)\n",
    "print(\"V\\n\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.15507899  2.45148915 -1.51024025 -0.63965575]\n",
      " [-0.73086231 -1.64426785  0.988813    0.98746207]\n",
      " [ 0.170691    0.61012234  0.88708605  0.87430013]\n",
      " [-0.33151822 -1.36325015 -0.08440168 -0.76657739]]\n",
      "1.2084942323232553 0.9952924737902545 1.2294349867684484\n"
     ]
    }
   ],
   "source": [
    "scaled=np.matmul(q,k.T) / math.sqrt(d_k)\n",
    "#without mask\n",
    "print(scaled)\n",
    "\n",
    "#dk sqrt needed to reduce variance\n",
    "print(q.var(), k.var(), scaled.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. -inf -inf -inf]\n",
      " [  0.   0. -inf -inf]\n",
      " [  0.   0.   0. -inf]\n",
      " [  0.   0.   0.   0.]]\n",
      "[[ 1.15507899        -inf        -inf        -inf]\n",
      " [-0.73086231 -1.64426785        -inf        -inf]\n",
      " [ 0.170691    0.61012234  0.88708605        -inf]\n",
      " [-0.33151822 -1.36325015 -0.08440168 -0.76657739]]\n"
     ]
    }
   ],
   "source": [
    "#masking\n",
    "mask = np.tril(np.ones((L,L)))\n",
    "mask[mask==0]=-np.inf\n",
    "mask[mask==1]=0\n",
    "print(mask)\n",
    "#with mask\n",
    "print(scaled+mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return(np.exp(x).T/np.sum(np.exp(x),axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20441969 0.74738865 0.01422294 0.03396873]\n",
      " [0.07962386 0.03194157 0.44451734 0.44391724]\n",
      " [0.15105971 0.23441817 0.30922534 0.30529678]\n",
      " [0.3045119  0.10852475 0.38987521 0.19708814]] \n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.71369654 0.28630346 0.         0.        ]\n",
      " [0.21744496 0.33743642 0.44511862 0.        ]\n",
      " [0.3045119  0.10852475 0.38987521 0.19708814]]\n"
     ]
    }
   ],
   "source": [
    "attention_enc=softmax(scaled)\n",
    "attention_dec=softmax(scaled + mask)\n",
    "print(attention_enc,\"\\n\",attention_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.42429686 -0.99272149 -0.72741752 -0.92203087 -0.15050227  0.80990414\n",
      "  -0.47904093  0.13621028]\n",
      " [ 0.15562979 -0.54832569  0.19981103 -0.82945797 -1.15858405  0.08286083\n",
      "  -1.63703714  0.42905453]\n",
      " [-0.32758327 -0.64660573 -0.05121693 -0.80298829 -0.89245908  0.26451627\n",
      "  -1.3048517   0.37893995]\n",
      " [-0.31663299 -0.2382044   0.33239546 -0.57589708 -1.25341791  0.08634792\n",
      "  -1.51227678  0.66633703]]\n",
      "[[-0.54636056 -0.15054645  0.25871746  0.59966612 -1.54475043 -0.67852663\n",
      "  -1.53219383  1.15456754]\n",
      " [-1.79208351 -1.21408589 -1.02846623 -1.34557521  0.27998331  1.27477948\n",
      "  -0.11694454 -0.14395123]\n",
      " [-0.36151134  0.83264214  1.40804037 -1.25752191 -1.81624049  0.58136874\n",
      "  -1.88049262  0.92802504]\n",
      " [ 0.93952905 -1.95460459 -0.93223817 -0.62001563 -0.53428377 -0.3655168\n",
      "  -1.52143447 -0.15949322]]\n"
     ]
    }
   ],
   "source": [
    "new_v=np.matmul(attention_enc,v)\n",
    "print(new_v)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v, mask=None):\n",
    "    d_k=q.shape[-1]\n",
    "    scaled=np.matmul(q,k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled=scaled+mask\n",
    "    attention=softmax(scaled)\n",
    "    out=np.matmul(attention,v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 7.74455694e-01  1.97064575e+00 -2.53153688e+00 -5.97745676e-01\n",
      "  -1.24164577e-01 -8.96424230e-01 -8.96535686e-01  1.55569502e+00]\n",
      " [ 5.71609011e-01 -2.49191484e+00  1.21663085e+00 -2.23144397e-02\n",
      "  -7.59877904e-01  6.56746304e-01  3.79476681e-01 -5.34879537e-01]\n",
      " [ 3.36630491e-01  3.08335569e-01  3.11152025e-01  2.62084597e-01\n",
      "  -1.61559422e+00 -1.59862759e-01 -6.39626461e-01  5.46925184e-02]\n",
      " [ 1.11802784e+00 -2.07663985e-03  5.47927395e-01  1.22459474e+00\n",
      "   1.40045173e+00  5.40624961e-01  9.90553466e-01 -2.05964003e+00]]\n",
      "K\n",
      " [[-0.31307806  0.37155654 -1.34929663 -0.6163584  -0.79354177 -1.01831504\n",
      "   0.52439403 -0.99514255]\n",
      " [ 0.4174169   2.09491864 -0.43264611 -1.4439124  -1.09854613  0.25280404\n",
      "   0.49942061  0.68256345]\n",
      " [-1.6419378  -1.19936668 -0.57121022  1.21714351 -1.76539886 -1.06801916\n",
      "  -0.58612409 -1.96506796]\n",
      " [ 0.33547963  0.26765436  0.91023886 -1.2619529  -1.74859269  0.94764148\n",
      "   0.54926845  0.05033437]]\n",
      "V\n",
      " [[-0.54636056 -0.15054645  0.25871746  0.59966612 -1.54475043 -0.67852663\n",
      "  -1.53219383  1.15456754]\n",
      " [-1.79208351 -1.21408589 -1.02846623 -1.34557521  0.27998331  1.27477948\n",
      "  -0.11694454 -0.14395123]\n",
      " [-0.36151134  0.83264214  1.40804037 -1.25752191 -1.81624049  0.58136874\n",
      "  -1.88049262  0.92802504]\n",
      " [ 0.93952905 -1.95460459 -0.93223817 -0.62001563 -0.53428377 -0.3655168\n",
      "  -1.52143447 -0.15949322]]\n",
      "new v\n",
      " [[-1.42429686 -0.99272149 -0.72741752 -0.92203087 -0.15050227  0.80990414\n",
      "  -0.47904093  0.13621028]\n",
      " [ 0.15562979 -0.54832569  0.19981103 -0.82945797 -1.15858405  0.08286083\n",
      "  -1.63703714  0.42905453]\n",
      " [-0.32758327 -0.64660573 -0.05121693 -0.80298829 -0.89245908  0.26451627\n",
      "  -1.3048517   0.37893995]\n",
      " [-0.31663299 -0.2382044   0.33239546 -0.57589708 -1.25341791  0.08634792\n",
      "  -1.51227678  0.66633703]]\n",
      "attention\n",
      " [[0.20441969 0.74738865 0.01422294 0.03396873]\n",
      " [0.07962386 0.03194157 0.44451734 0.44391724]\n",
      " [0.15105971 0.23441817 0.30922534 0.30529678]\n",
      " [0.3045119  0.10852475 0.38987521 0.19708814]]\n",
      "new v\n",
      " [[-0.54636056 -0.15054645  0.25871746  0.59966612 -1.54475043 -0.67852663\n",
      "  -1.53219383  1.15456754]\n",
      " [-0.90301535 -0.45504148 -0.10980769  0.04273679 -1.02232284 -0.11928832\n",
      "  -1.12700306  0.78279712]\n",
      " [-0.88443302 -0.07178784  0.33595983 -0.88339812 -1.04986409  0.54139288\n",
      "  -1.20967146  0.61556173]\n",
      " [-0.31663299 -0.2382044   0.33239546 -0.57589708 -1.25341791  0.08634792\n",
      "  -1.51227678  0.66633703]]\n",
      "attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.71369654 0.28630346 0.         0.        ]\n",
      " [0.21744496 0.33743642 0.44511862 0.        ]\n",
      " [0.3045119  0.10852475 0.38987521 0.19708814]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q,k,v, mask=None)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"new v\\n\", values)\n",
    "print(\"attention\\n\", attention)\n",
    "values2, attention2 = scaled_dot_product_attention(q,k,v, mask=mask)\n",
    "\n",
    "print(\"new v\\n\", values2)\n",
    "print(\"attention\\n\", attention2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
